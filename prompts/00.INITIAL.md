# AI-Native Infrastructure Harness

Build a harness that lets an AI agent develop and operate an infrastructure service autonomously.

## The Core Idea

Infrastructure services should be built and operated by AI agents. Not AI inside the service—the service stays deterministic and testable. AI operates *on* the service: writing code, deploying changes, responding to issues, improving performance.

The harness is the scaffolding that makes this possible.

## Architecture

Four processes, loosely coupled through a database and Grafana Cloud:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              PROCESSES                                       │
│                                                                             │
│  ┌───────────────┐ ┌───────────────┐ ┌───────────────┐ ┌─────────────────┐ │
│  │     web       │ │    monitor    │ │     agent     │ │    service      │ │
│  │               │ │               │ │               │ │                 │ │
│  │ Dashboard,    │ │ Watches       │ │ Works         │ │ The thing       │ │
│  │ APIs for      │ │ metrics,      │ │ tickets,      │ │ being managed   │ │
│  │ humans        │ │ creates       │ │ calls Claude, │ │ (rate limiter)  │ │
│  │               │ │ tickets       │ │ takes action  │ │                 │ │
│  └───────────────┘ └───────────────┘ └───────────────┘ └─────────────────┘ │
│          │                 │                 │                   │          │
│          └────────────┬────┴─────────────────┴───────────────────┘          │
│                       ▼                                                     │
│            ┌─────────────────────┐                                          │
│            │      Database       │                                          │
│            │  (tickets, SLOs,    │                                          │
│            │   invariants, etc)  │                                          │
│            └─────────────────────┘                                          │
└─────────────────────────────────────────────────────────────────────────────┘
                        │
                        │ queries / emits
                        ▼
             ┌─────────────────────────┐
             │    Grafana Cloud        │
             │  (Prometheus + Loki)    │
             └─────────────────────────┘
```

**web** — Human interface. Dashboard showing system state, APIs for CRUD on tickets/SLOs/invariants. No AI here.

**monitor** — The eyes. Continuously evaluates SLOs and invariants by querying Grafana Cloud. Creates tickets when things go wrong. Also handles scheduled checks and external webhooks.

**agent** — The hands. Picks up tickets, calls Claude with tools, executes actions (code edits, deploys, queries), updates ticket status. This is where AI lives.

**service** — The subject. A real infrastructure service (start with a rate limiter). Emits metrics and logs to Grafana Cloud. Knows nothing about the harness.

## Key Abstractions

### Tickets

The universal unit of work. Everything flows through tickets.

A ticket has:
- An objective (what to achieve)
- Success criteria (how to verify, tied to metrics)
- Context (related info)
- Status (pending, in_progress, completed, failed)
- Execution history (what the agent did)

Tickets come from multiple sources:
- SLO violations (burn rate too high)
- Invariant violations (condition failed)
- Anomaly detection (statistical deviation)
- Scheduled checks (periodic maintenance)
- Human requests (via dashboard)
- External webhooks (other alerting systems)

**Key insight:** Alerts don't page humans. Alerts create tickets. The agent works the ticket.

### SLOs

Commitments to users. Availability, latency, error rates.

SLOs have:
- A target (e.g., 99.9% availability)
- A time window (e.g., 30 days rolling)
- An error budget (derived from target and window)
- Burn rate thresholds (when to alert)

When burn rate exceeds thresholds, create a ticket. Fast burn (14x) = high priority. Slow burn but budget depleting = medium priority.

### Invariants

Internal operational conditions that must always hold.

Examples:
- Capacity headroom > 20%
- No stale connections
- Config consistent across replicas

Binary pass/fail. Any violation creates a ticket immediately.

**SLOs protect users. Invariants protect operations.**

### Trajectories

Every ticket execution is a trajectory. Every trajectory is training data.

Capture:
- Initial state (metrics, objective)
- Each step (action taken, observation received)
- Final outcome (success/failure, metrics after)

This is a laboratory for infrastructure-focused reinforcement learning. The same system that runs production generates training data for better models.

## The Agent

The agent works tickets by calling Claude with tools. Tools let Claude:

**Observe:**
- Query Prometheus (metrics)
- Query Loki (logs)
- Read files
- Search code

**Act:**
- Edit files (creates git commits)
- Run tests
- Deploy services
- Update configuration
- Create new tickets
- Add notes to tickets

The agent loop:
1. Pick a pending ticket
2. Build context (objective, success criteria, relevant code/metrics)
3. Call Claude with tools
4. Execute tool calls, feed results back
5. Repeat until Claude says done or gives up
6. Update ticket status, record trajectory

Some actions require human approval (e.g., production deploys, large changes). The agent flags these and waits.

## The Monitor

The monitor runs ticket generators on schedule:
- SLO evaluation (every minute or so)
- Invariant evaluation (every minute or so)  
- Anomaly detection (every few minutes)
- Scheduled checks (cron-like)

It queries Grafana Cloud, compares against thresholds, creates tickets when needed.

## The First Service: Rate Limiter

Start with something simple but real. A token bucket rate limiter:
- Clients request tokens
- Service allows or denies based on bucket state
- Emits metrics (requests, latency, denials)
- Emits structured logs

This gives the agent something to optimize, debug, and improve.

## Technology Choices

**Suggestions, not requirements:**

- Python (fast to iterate)
- FastAPI (web framework)
- SQLite (database, keep it simple)
- Grafana Cloud Free Tier (observability, no infra to manage)
- Claude API (the brain)

Use whatever makes sense. The architecture matters more than the stack.

## Success Criteria

The demo succeeds when:

1. **Autonomous operation** — The rate limiter runs for a week with no human intervention (no SSH, no manual fixes, no direct code changes)

2. **AI-initiated improvements** — At least 3 tickets created by the system (not humans) that result in measurable improvements

3. **Visible loop** — Dashboard shows the complete cycle: detection → ticket → action → verification

4. **Audit trail** — Every change traceable to a ticket, every ticket traceable to an observation

5. **Trajectory export** — Can export execution histories for training

## What To Build First

Suggested order (adjust as needed):

**Week 1: Foundation**
- Database schema (tickets, SLOs, invariants)
- Basic web dashboard
- Grafana Cloud integration (query helpers)

**Week 2: Monitor**
- SLO evaluator
- Invariant evaluator
- Ticket creation from violations

**Week 3: Agent**
- Tool definitions
- Agent loop
- Claude integration
- Basic actions (read, edit, test)

**Week 4: Service + Polish**
- Rate limiter with metrics/logs
- Deploy action
- Dashboard enhancements
- Trajectory capture and export

## Principles

**Hermetic** — Everything observable and controllable through APIs. No hidden state, no out-of-band actions.

**Ticket-driven** — All work flows through tickets. Structured objectives, measurable success criteria.

**Auditable** — Every action logged, every change traced to a ticket.

**Separation** — AI operates on the service, not in the service. The service stays deterministic.

## Open Questions

Things to figure out as you build:

- How much context does Claude need per ticket?
- When should actions require approval?
- How to handle cascading failures?
- What's the right granularity for trajectories?
- How to prioritize competing tickets?

Don't over-plan. Build, learn, adjust.
